{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST DCGAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WNIk7VHnTWnW","colab_type":"text"},"cell_type":"markdown","source":["### Install and import require packages"]},{"metadata":{"id":"8zGbENOJgnRm","colab_type":"code","colab":{}},"cell_type":"code","source":["import os, time\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import itertools\n","import pickle\n","import imageio\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","from torchvision import datasets, transforms"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aA-2FTTkTNWY","colab_type":"text"},"cell_type":"markdown","source":["### Set number of epochs, learning rate and batch size"]},{"metadata":{"id":"uBXj5bpmgnRr","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","lr = 0.0002\n","num_of_epochs = 20"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fQj-aXdvgnRw","colab_type":"text"},"cell_type":"markdown","source":["### Load Dataset"]},{"metadata":{"id":"j_HC4nJQgnRx","colab_type":"code","colab":{}},"cell_type":"code","source":["img_size = 64\n","transform = transforms.Compose([\n","        transforms.Scale(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data', train=True, download=True, transform=transform),\n","    batch_size=batch_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"abqagWrMgnR2","colab_type":"text"},"cell_type":"markdown","source":["### Model"]},{"metadata":{"id":"KOWkeJY3gnR4","colab_type":"code","colab":{}},"cell_type":"code","source":["class ImageDiscriminator(nn.Module):\n","    def __init__(self, noise_vector_size=128):\n","        super(ImageDiscriminator, self).__init__()\n","        ##TODO: implement initial 4 Convolution layers using noise_vector_size with linear kernel(=4), stride(=2) and padding(=1)##\n","        ##TODO: add BatchNorm layers from second conv layer till fourth conv layer##\n","        self.conv5 = nn.Conv2d(noise_vector_size * 8, 1, 4, 1, 0)\n","\n","    def weight_init(self, mean, std):\n","        for m in self._modules:\n","            normal_init(self._modules[m], mean, std)\n","\n","    def forward(self, input):\n","        ##TODO: implement forward pass (be cautious when adding BatchNorm layers)##\n","        x = F.sigmoid(self.conv5(x))\n","\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"96PQrpb4gnR8","colab_type":"code","colab":{}},"cell_type":"code","source":["class ImageGenerator(nn.Module):\n","    def __init__(self, noise_vector_size=128):\n","        super(ImageGenerator, self).__init__()\n","        self.deconv1 = nn.ConvTranspose2d(100, noise_vector_size * 8, 4, 1, 0)\n","        ##TODO: implement rest 4 Tranpose Convolution layers using noise_vector_size with linear kernel(=4), stride(=2) and padding(=1)##\n","        ##TODO: add BatchNorm layers from first conv layer till fourth conv layer##\n","\n","    # weight_init\n","    def weight_init(self, mean, std):\n","        for m in self._modules:\n","            normal_init(self._modules[m], mean, std)\n","\n","    # forward method\n","    def forward(self, input):\n","        ##TODO: implement forward pass (be cautious when adding BatchNorm layers)##\n","        x = F.tanh(self.deconv5(x))\n","\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ItictmBagnSB","colab_type":"code","colab":{}},"cell_type":"code","source":["###TODO: Replace normal weight initialization with xavier intialization##\n","def normal_init(m, mean, std):\n","    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n","        m.weight.data.normal_(mean, std)\n","        m.bias.data.zero_()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pc4CfG16gnSH","colab_type":"code","colab":{}},"cell_type":"code","source":["fixed_noise = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)\n","fixed_noise = Variable(fixed_noise.cuda(), volatile=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Fbdxvl6hgnSR","colab_type":"code","colab":{}},"cell_type":"code","source":["def show_epoch_result(num_epoch, show = False, save = False, path = 'result.png', isFix=False):\n","    random_noise = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n","    random_noise = Variable(random_noise.cuda(), volatile=True)\n","\n","    G.eval()\n","    if isFix:\n","        test_images = G(fixed_noise)\n","    else:\n","        test_images = G(random_noise)\n","    G.train()\n","\n","    size_figure_grid = 5\n","    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n","    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n","        ax[i, j].get_xaxis().set_visible(False)\n","        ax[i, j].get_yaxis().set_visible(False)\n","\n","    for k in range(5*5):\n","        i = k // 5\n","        j = k % 5\n","        ax[i, j].cla()\n","        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap='gray')\n","\n","    label = 'Epoch {0}'.format(num_epoch)\n","    fig.text(0.5, 0.04, label, ha='center')\n","    plt.savefig(path)\n","\n","    if show:\n","        plt.show()\n","    else:\n","        plt.close()\n","\n","\n","def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n","    ##TODO: implement show_train_hist function to plot losses histograms ##\n","\n","    if save:\n","        plt.savefig(path)\n","\n","    if show:\n","        plt.show()\n","    else:\n","        plt.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L-QReeRIgnSW","colab_type":"code","colab":{}},"cell_type":"code","source":["G = ImageGenerator(128)\n","D = ImageDiscriminator(128)\n","G.weight_init(mean=0.0, std=0.02)\n","D.weight_init(mean=0.0, std=0.02)\n","G.cuda()\n","D.cuda()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qrdXIaShgnSc","colab_type":"text"},"cell_type":"markdown","source":["### Optimization"]},{"metadata":{"id":"zonkoFbKgnSe","colab_type":"code","colab":{}},"cell_type":"code","source":["BCE_loss = nn.BCELoss()\n","G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n","D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fCSi5CXRv-Ke","colab_type":"text"},"cell_type":"markdown","source":["### Results folder to save visualizations"]},{"metadata":{"id":"fasnUi8PgnSk","colab_type":"code","colab":{}},"cell_type":"code","source":["if not os.path.isdir('results'):\n","    os.mkdir('results')\n","if not os.path.isdir('results/dcgan'):\n","    os.mkdir('results/dcgan')\n","if not os.path.isdir('results/dcgan/Random_results'):\n","    os.mkdir('results/dcgan/Random_results')\n","if not os.path.isdir('results/dcgan/Fixed_results'):\n","    os.mkdir('results/dcgan/Fixed_results')\n","\n","train_hist = {}\n","train_hist['D_losses'] = []\n","train_hist['G_losses'] = []\n","train_hist['per_epoch_times'] = []\n","train_hist['total_time'] = []\n","num_iter = 0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YYIWm2fLgnSo","colab_type":"text"},"cell_type":"markdown","source":["### Training"]},{"metadata":{"id":"KdR-uCUxgnSp","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Started training...')\n","start_time = time.time()\n","for epoch in range(num_of_epochs):\n","    D_losses = []\n","    G_losses = []\n","    epoch_start_time = time.time()\n","    for x_, _ in train_loader:\n","        ##TODO: fill-in training discriminator procedure (hint: similar to mlp_gan.ipynb implementation) ##\n","        D_losses.append(D_train_loss.item())\n","\n","        ##TODO: fill-in training generator procedure (hint: similar to mlp_gan.ipynb implementation) ##\n","        G_losses.append(G_train_loss.item())\n","\n","        num_iter += 1\n","\n","    epoch_end_time = time.time()\n","    per_epoch_time = epoch_end_time - epoch_start_time\n","\n","\n","    print('[%d/%d] - time: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), num_of_epochs, per_epoch_time, torch.mean(torch.FloatTensor(D_losses)),\n","                                                              torch.mean(torch.FloatTensor(G_losses))))\n","    p = 'results/dcgan/Random_results/' + str(epoch + 1) + '.png'\n","    fixed_p = 'results/dcgan/Fixed_results/' + str(epoch + 1) + '.png'\n","    \n","    ##TODO: save the epoch results with fixed_noise and randon_noise ##\n","    \n","    train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n","    train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n","    train_hist['per_epoch_times'].append(per_epoch_time)\n","\n","end_time = time.time()\n","total_time = end_time - start_time\n","train_hist['total_time'].append(total_time)\n","\n","print(\"Avg per epoch time: %.2f, total %d epochs time: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_times'])), num_of_epochs, total_time))\n","print(\"Training finish!... save training results\")\n","\n","##TODO: save the generator and discriminator weights into .pkl files ##\n","\n","with open('results/dcgan/train_hist.pkl', 'wb') as f:\n","    pickle.dump(train_hist, f)\n","\n","##TODO: save training histograms ##"],"execution_count":0,"outputs":[]}]}